import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader

def run_rl_training(dataset, encoder, agent, config):
    # 1. å½»åº•é”å®š Encoderï¼ŒRL åªè®­ç»ƒå†³ç­–å¤´ (Agent)
    encoder.eval()
    for param in encoder.parameters():
        param.requires_grad = False
        
    device = "cuda" if torch.cuda.is_available() else "cpu"
    optimizer = torch.optim.AdamW(agent.parameters(), lr=3e-4)
    loader = DataLoader(dataset, batch_size=config.get('batch_size', 1024), shuffle=True)
    
    print(f"ğŸ“ˆ å¼€å§‹ RL è®­ç»ƒ (IQN ç®—æ³•)...")
    
    for epoch in range(config.get('epochs', 50)):
        total_loss = 0
        for batch in loader:
            # è·å– Anchor åºåˆ—ä½œä¸ºå½“å‰çŠ¶æ€ S
            x = batch["anchor"].to(device)
            # è¿™é‡Œç®€åŒ–æ¼”ç¤ºï¼Œä½¿ç”¨ y_future ä½œä¸ºå¥–åŠ± R (å®é™…åº”æ ¹æ® Action è®¡ç®—æ”¶ç›Š)
            rewards = batch["y_future"].to(device) 
            
            # 2. æå–å¹¶ç¨³å®š Latent (Industrial Patch)
            with torch.no_grad():
                latent, _, _ = encoder(x)
                # æ¶ˆé™¤åˆ†å¸ƒæ¼‚ç§»å¹¶é™åˆ¶ç¦»ç¾¤å€¼
                latent = F.layer_norm(latent, (latent.size(-1),))
                latent = torch.clamp(latent, -3.0, 3.0)
            
            # 3. è®¡ç®— IQN æŸå¤± (åˆ†ä½æ•°å›å½’)
            # é‡‡æ ·ä¸¤ç»„ä¸åŒçš„åˆ†ä½æ•° taus
            taus = torch.rand(x.size(0), config['K']).to(device)
            # è·å–å½“å‰çŠ¶æ€çš„ Q åˆ†å¸ƒ
            current_q_dist = agent.get_q_dist(latent, taus) 
            
            # ç®€åŒ–ç‰ˆ Huber Loss ç›®æ ‡ (é’ˆå¯¹é‡‘èæ•°æ®çš„åšå°¾ç‰¹æ€§)
            # å®é™… RL ä¸­éœ€å¯¹æ¯” Q(s,a) ä¸ R + Q(s', a')ï¼Œæ­¤å¤„ä»¥æ‹Ÿåˆé¢„æœŸæ”¶ç›Šä¸ºä¾‹
            diff = rewards.unsqueeze(1) - current_q_dist # [B, K, Action]
            loss = (torch.abs(taus.unsqueeze(-1) - (diff < 0).float()) * F.huber_loss(diff, torch.zeros_like(diff), reduction='none')).mean()

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(agent.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            
        print(f"Epoch {epoch} | RL Loss: {total_loss/len(loader):.6f}")

    # 4. ä¿å­˜ Agent æƒé‡
    torch.save(agent.state_dict(), "models/agent_latest.pth")
    print("âœ… RL Agent è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜ã€‚")