import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from core.backbone import MambaEncoder
from core.loss import IndustrialRiskLoss
from data.pipeline import RegimeAwareDataset
import os

# --- 1. é…ç½®è¶…å‚æ•° ---
CONFIG = {
    'd_model': 128,
    'n_layers': 4,
    'batch_size': 512,  # è€ƒè™‘åˆ° Triplet ä¼šå ç”¨ 3 å€æ˜¾å­˜ï¼Œå»ºè®®ä» 512 å¼€å§‹
    'epochs': 50,
    'lr': 1e-4,
    'weight_decay': 1e-5,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'save_path': './models/encoder_latest.pth'
}

def train_pretrain(stocks_tensors):
    device = CONFIG['device']
    
    # --- 2. å‡†å¤‡ç»„ä»¶ ---
    # åˆå§‹åŒ– Dataset & DataLoader
    dataset = RegimeAwareDataset(stocks_tensors, seq_len=60, lookahead=5)
    dataloader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4)
    
    # åˆå§‹åŒ–æ¨¡å‹ä¸æŸå¤±å‡½æ•°
    model = MambaEncoder(d_model=CONFIG['d_model'], n_layers=CONFIG['n_layers']).to(device)
    criterion = IndustrialRiskLoss(quantiles=[0.01, 0.05, 0.5, 0.95, 0.99]).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])
    
    # å»ºç«‹æ¨¡å‹ç›®å½•
    os.makedirs('./models', exist_ok=True)
    
    print(f"ğŸ”¥ é¢„è®­ç»ƒå¯åŠ¨ï¼è®¾å¤‡: {device}, æ ·æœ¬æ€»æ•°: {len(dataset)}")

    for epoch in range(CONFIG['epochs']):
        model.train()
        total_loss = 0
        
        # åŠ¨æ€è®¡ç®— Loss æƒé‡ (Annealing ç­–ç•¥)
        # å‰æœŸä¸»æ”» Diffusion (w1), ä¸­æœŸå¼•å…¥ Regime (w3), åæœŸå¼ºåŒ– Tail Risk (w2)
        w1 = 1.0 # Diffusion å§‹ç»ˆä¿æŒ
        w2 = min(1.0, epoch / (CONFIG['epochs'] * 0.5)) # Tail Risk æƒé‡é€æ¸å¢åŠ 
        w3 = 0.5 # å¯¹æ¯”å­¦ä¹ ä¿æŒå¸¸æ€
        weights = [w1, w2, w3]

        for i, batch in enumerate(dataloader):
            # è·å– Triplet æ ·æœ¬
            anchor = batch['anchor'].to(device) # [B, 60, D]
            pos = batch['pos'].to(device)
            neg = batch['neg'].to(device)
            y_future = batch['y_future'].to(device).unsqueeze(1)
            noise_true = batch['noise'].to(device)

            # --- 3. å‰å‘ä¼ æ’­ ---
            # ä¸ºäº†è®¡ç®— InfoNCEï¼Œæˆ‘ä»¬éœ€è¦ä¸‰è€…çš„ Latents
            # ä½†åªæœ‰ Anchor éœ€è¦è®¡ç®— Quantile å’Œ Diffusion Head
            z_anchor, q_pred, diff_out = model(anchor)
            z_pos, _, _ = model(pos)
            z_neg, _, _ = model(neg)

            # --- 4. è®¡ç®—ç»„åˆ Loss ---
            # è¿™é‡Œçš„ z_tuple æ˜¯ (anchor, pos, neg)
            l_diff, l_tail, l_regime = criterion(
                diff_out, noise_true,     # Diffusion é¡¹
                q_pred, y_future,         # Pinball é¡¹
                (z_anchor, z_pos, z_neg), # InfoNCE é¡¹
                weights
            )
            
            loss = l_diff + l_tail + l_regime

            # --- 5. ä¼˜åŒ– ---
            optimizer.zero_grad()
            loss.backward()
            
            # å·¥ä¸šçº§è¡¥ä¸ï¼šæ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢ Mamba åœ¨å¤„ç†æç«¯åºåˆ—æ—¶æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()

            if i % 50 == 0:
                print(f"Epoch [{epoch}/{CONFIG['epochs']}] Step [{i}/{len(dataloader)}] "
                      f"Loss: {loss.item():.4f} (Diff: {l_diff.item():.4f}, "
                      f"Tail: {l_tail.item():.4f}, Regime: {l_regime.item():.4f})")

        # æ¯ä¸ª Epoch ç»“æŸåä¿å­˜ä¸€æ¬¡
        torch.save(model.state_dict(), CONFIG['save_path'])
        print(f"âœ¨ Epoch {epoch} å®Œæˆï¼Œå¹³å‡ Loss: {total_loss / len(dataloader):.4f}")

if __name__ == "__main__":
    # è¿™é‡Œæ¥å…¥ä½ ä¹‹å‰ load_all_csv å¾—åˆ°çš„ stocks_tensors
    # stocks_tensors = loader.load_all_csv()
    # train_pretrain(stocks_tensors)
    pass